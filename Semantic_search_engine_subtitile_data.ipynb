{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2f695d-ea4b-4302-8a12-9828393aba3b",
   "metadata": {},
   "source": [
    "## Background: \n",
    "In the fast-evolving landscape of digital content, effective search engines play a pivotal role in connecting users with relevant information. For Google, providing a seamless and accurate search experience is paramount. This project focuses on improving the search relevance for video subtitles, enhancing the accessibility of video content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f142a-1973-4f7e-9960-2e5a025ccbc0",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "Develop an advanced search engine algorithm that efficiently retrieves subtitles based on user queries, with a specific emphasis on subtitle content. The primary goal is to leverage natural language processing and machine learning techniques to enhance the relevance and accuracy of search results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90981ab4-1ff0-42ce-9b6b-7599742bd7df",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e2a0ea-1aaf-4ad4-ac19-8cb23bcc38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd092b0-d870-4319-9533-168fefa04a02",
   "metadata": {},
   "source": [
    "# Step 1 - Reading the Tables from Database file****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1c39c4-6cc6-4015-84ca-f7d217112531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x17507e7c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Read the code below and write your observation in the next cell\n",
    "\n",
    "conn = sqlite3.connect(\"eng_subtitles_database.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7345552-3bce-401e-bebf-dca0f5f79cd3",
   "metadata": {},
   "source": [
    "### In the above cell, I am able to read the table inside the database. As mentioned earlier, table name is zipfiles. We also know from README.txt that this table contains three columns: 'num', 'name' and 'content'.****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504c72a-d9da-4640-b85a-93773d8a976d",
   "metadata": {},
   "source": [
    "# Step 2 - Reading the columns of Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07439399-bfcb-4a47-948c-44eec87f84e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x17507e7c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"PRAGMA table_info('zipfiles')\")\n",
    "cols = cursor.fetchall()\n",
    "for col in cols:\n",
    "    print(col[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad2201-120a-483e-bbca-4f4a9de6c043",
   "metadata": {},
   "source": [
    "### The above code helps in checking the column names in the database table.\n",
    "Let's now use SELECT * FROM zipfiles to read all the data into a df variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6fdbe-b294-42d2-a23b-560cdfc446f0",
   "metadata": {},
   "source": [
    "# Step 3 - Loading the Database Table inside a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "310626b3-3a61-4d4f-9665-ed2f21c4603e",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM zipfiles': no such table: zipfiles",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:2202\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2202\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: zipfiles",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql_query(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mSELECT * FROM zipfiles\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m, conn)\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:469\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m    466\u001b[0m     dtype_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[1;32m    470\u001b[0m         sql,\n\u001b[1;32m    471\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m    472\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    473\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[1;32m    474\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[1;32m    475\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    476\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    477\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    478\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:2266\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2257\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2266\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[1;32m   2267\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:2214\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2213\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2214\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM zipfiles': no such table: zipfiles"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT * FROM zipfiles\"\"\", conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d99e3e-809c-4df1-86c6-2e86909b5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031dd829-7d6e-41ae-9d4c-fdb1c6489146",
   "metadata": {},
   "source": [
    "### Looks like the content column do not contain the subtitles text. Instead as mentioned in README.txt, it might be latin-1 encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fbccc-982a-442b-a6f0-7c6c2b9b53d4",
   "metadata": {},
   "source": [
    "# Step 4 - Printing content of 0th Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf67e3b-856f-4e7d-9f66-1fe1cbac09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data = df.iloc[0, 2]\n",
    "\n",
    "# here 2 represent the index of content column\n",
    "# 0 represents the row number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf20970-acd2-4a55-97ca-952fbcc36779",
   "metadata": {},
   "source": [
    "###  From the content, it appears to start with the bytes \"PK\\x03......\", which suggests that it might be a ZIP archive file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fad292-9ef8-400c-8bcd-879a507c239d",
   "metadata": {},
   "source": [
    "# Step 5 - Unzipping the content of 385th row and decoding using latin-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1940f-771b-4530-965f-e49f598010da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Assuming 'content' is the binary data from your database\n",
    "binary_data = df.iloc[385, 2]\n",
    "\n",
    "# Decompress the binary data using the zipfile module\n",
    "with io.BytesIO(binary_data) as f:\n",
    "    with zipfile.ZipFile(f, 'r') as zip_file:\n",
    "        # Reading only one file in the ZIP archive\n",
    "        subtitle_content = zip_file.read(zip_file.namelist()[0])\n",
    "\n",
    "# Now 'subtitle_content' should contain the extracted subtitle content\n",
    "print(subtitle_content.decode('latin-1'))  # Assuming the content is latin-1 encoded text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f1375-8f48-47f5-8b88-3a91723835ff",
   "metadata": {},
   "source": [
    "# Step 6 - Applying the above Function on the Entire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4834cf-6fd2-4213-ab47-4c56956257e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "\n",
    "count = 0\n",
    "\n",
    "def decode_method(binary_data):\n",
    "    global count\n",
    "    # Decompress the binary data using the zipfile module\n",
    "    # print(count, end=\" \")\n",
    "    count += 1\n",
    "    with io.BytesIO(binary_data) as f:\n",
    "        with zipfile.ZipFile(f, 'r') as zip_file:\n",
    "            # Assuming there's only one file in the ZIP archive\n",
    "            subtitle_content = zip_file.read(zip_file.namelist()[0])\n",
    "\n",
    "    # Now 'subtitle_content' should contain the extracted subtitle content\n",
    "    return subtitle_content.decode('latin-1')  # Assuming the content is UTF-8 encoded text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df4189-fcf7-47c9-b634-e3ab2fc483ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['file_content'] = df['content'].apply(decode_method)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58047b02-03c1-4524-9ebf-1d8050265298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777cb0e-7e78-40b6-8d4c-ed58a875edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382ca28-3f3d-4c2d-91d4-98b8ee75eb50",
   "metadata": {},
   "source": [
    "# Step 7- Slice the DataFrame to get 30% of the data and store it in another DataFrame using the iloc method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c030d16-2b3e-46e4-b6b5-528a7e57a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data = df[:26000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0fe91-686b-4c01-82fc-b87b242e2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2bcb8-4a14-4c55-8330-16ac105e1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9596a7-beb3-4fe8-a67a-c540273a1591",
   "metadata": {},
   "source": [
    "# Data Preprocessing¶\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fae808-a4b3-4536-a511-cf8ff409acc6",
   "metadata": {},
   "source": [
    "# Step 1 : Removing the timestamp from file_content column using regexx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75050973-75f5-448c-bc91-e12698a7bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the regex pattern\n",
    "pattern = r'\\d{2}:\\d{2}:\\d{2},\\d{3}\\s*-->\\s*\\d{2}:\\d{2}:\\d{2},\\d{3}\\s*'\n",
    "\n",
    "# Apply the regex pattern to the specified column\n",
    "sliced_data['cleaned_text'] = sliced_data['file_content'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "sliced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40dc97-0d0f-4dae-9a15-a23afd93dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b59758-52f6-4984-bdea-cf19c5527c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # remove timestamps from subtitle documents\n",
    "    cleaned_text = re.sub(r'\\d+:\\d+:\\d+,\\d+ --> \\d+:\\d+:\\d+,\\d+', '', text)\n",
    "    # Remove line numbers\n",
    "    cleaned_text = re.sub(r'\\d+\\s*', '', text)\n",
    "    # Remove HTML tags\n",
    "    cleaned_text = BeautifulSoup(cleaned_text, \"html.parser\").get_text(separator=\" \")\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'[ï]', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'[âª]', '', cleaned_text)\n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Join tokens back into text\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text.strip()\n",
    "\n",
    "# Apply preprocessing to 'content' column\n",
    "sliced_data['processed_content'] = sliced_data['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(sliced_data['processed_content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2da804-562a-45f3-88a9-fa389e90654b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sliced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd431ba6-1cf3-48cb-8537-710e6746d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data = sliced_data.drop('content', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d329b6-5302-4531-a8a4-51414a945dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b3128-c94a-4c51-a60b-d71b65025dd1",
   "metadata": {},
   "source": [
    "# Document Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06969d-f8c3-47b9-b903-8127d56d7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(text, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    words = word_tokenize(text)\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "#Apply chunking to each subtitle document\n",
    "chunked_data = sliced_data['processed_content'].apply(chunk_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe53a1-e95a-4bd9-9858-e505a461dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb688d-9ff6-47d8-a1aa-cf3636350754",
   "metadata": {},
   "source": [
    "# Saving the Chunked Subtitle Data in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449ee7f-3acc-468f-b42e-18694dd16569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path for the CSV file\n",
    "output_csv2_file = 'cleaned_chunked_subtitle_data.csv'\n",
    "\n",
    "# Write the 'cleaned_text' column to a CSV file\n",
    "sliced_data.to_csv(output_csv2_file, index=False, header=True)\n",
    "\n",
    "print(f\"Cleaned subtitle data has been saved to {output_csv2_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5dd45-0f7b-4d62-8baa-0c519e7ddc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('cleaned_chunked_subtitle_data.csv')\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a9891-0ba5-4ab7-abc3-1c374b175959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117fca6f-ea8a-40e2-b86c-dbffe9d32d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data.iloc[0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab2044-05f2-434d-b47c-add74739b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc08261-aa4e-4d91-aff9-e27316d3ba14",
   "metadata": {},
   "source": [
    "# Generating Text Vectors Using BERT based Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f654b7c-031e-4779-97ea-95e8922d7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e037ce9-2be2-417b-b63e-638a31a3ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data['doc_vector_pretrained_bert'] = sliced_data.processed_content.apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05fecbe-a6d3-416c-9fdb-5c536344c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85764e7-966e-429f-9b04-86a20c0cce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data.to_csv('search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('search.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1432123-87e1-4a4c-8a34-db954c66471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6e73f-0873-417a-9dfd-3c2d72d1c906",
   "metadata": {},
   "source": [
    "# Creating Query Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441b67c-6945-4320-bd37-2769f3d33177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, data, embeddings, model):\n",
    "    \n",
    "    query_embedding = model.encode([query])[0]\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)\n",
    "    \n",
    "    top_n = 10\n",
    "    top_indices = np.argsort(similarities[0])[-top_n:][::-1]  \n",
    "    results = [(data['name'][i], similarities[0][i]) for i in top_indices]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed84107-7673-4efd-8ccd-e1cc2df2e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(sliced_data['doc_vector_pretrained_bert'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7a451-c818-4e69-ac8b-813b9ff58198",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    embedding_dict[i] = embedding\n",
    "\n",
    "for i in range(1):\n",
    "    print(f\"Embedding {i}: {embedding_dict[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40400df-06eb-4ed4-bd6a-45a4c15990e7",
   "metadata": {},
   "source": [
    "# Calculating Cosine Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e73974-d425-4bc5-91da-8da8bbba8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = input(\"Enter your search query of English movies and series:\")\n",
    "search_results = search(query, sliced_data, embeddings, model)\n",
    "for result in search_results:\n",
    "    print(\"Document:\", result[0])\n",
    "    print(\"Similarity Score:\", result[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1119cd-ea7d-4aa1-898c-4af325f9bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = sliced_data.index.astype(str).tolist()\n",
    "documents = sliced_data['processed_content'].tolist()\n",
    "metadata = sliced_data.drop(['file_content','cleaned_text','processed_content','doc_vector_pretrained_bert'], axis = 1).to_dict(orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d23bd04-6d10-4ca0-b55e-39eb246e9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8887bed-a45a-44c8-83ec-e5a33baec78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55697c3b-9122-49b3-a1b8-a8cd3896714a",
   "metadata": {},
   "source": [
    "# Storing the Vectors generated using ChromaDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3932c5-02d7-4b8d-88b4-eed7ae010913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672fd39-689d-47f2-899a-37044c6fb18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(name=\"SubtitleSearch_Engine\", metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ddfb5-8d7f-49a5-9324-ca47b5164bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_as_lists = [embedding.tolist() for embedding in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed9488-3f90-419f-91ce-b59d27018bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, embedding in enumerate(embeddings_as_lists):\n",
    "\n",
    "    # Add the embeddings list to your collection\n",
    "    collection.add(\n",
    "            documents=documents[i],\n",
    "            embeddings=embeddings_as_lists[i],\n",
    "            ids=ids[i],\n",
    "            metadatas=metadata[i]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b933d-8d60-42cb-9368-139783f3a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=[\" through abraham noah moses and through jesus christ 571 why should we be so surprised that god speaks to u now through muhammad 572 who taught you those name 573 they are named in the quran\"],\n",
    "                           n_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec71bb8-a61a-4775-978a-8d1d6e1412b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
